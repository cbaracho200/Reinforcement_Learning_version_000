# ðŸŽ¯ COMO USAR O BUSINESS-RL

## ðŸ“š Ãndice
1. [Como Desenvolver Modelos Passo a Passo](#-como-desenvolver-modelos-passo-a-passo)
2. [InstalaÃ§Ã£o](#instalaÃ§Ã£o)
3. [Primeiro Exemplo (BÃ¡sico)](#primeiro-exemplo-bÃ¡sico)
4. [Exemplo IntermediÃ¡rio](#exemplo-intermediÃ¡rio)
5. [Exemplo AvanÃ§ado](#exemplo-avanÃ§ado)
6. [Problemas PrÃ©-ConstruÃ­dos](#problemas-prÃ©-construÃ­dos)
7. [API Completa](#api-completa)
8. [Dicas e Boas PrÃ¡ticas](#dicas-e-boas-prÃ¡ticas)

---

## ðŸŽ“ Como Desenvolver Modelos Passo a Passo

### VisÃ£o Geral do Processo

Desenvolver um modelo de Reinforcement Learning requer seguir um processo estruturado. Aqui estÃ¡ o guia completo:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  1. Entender o Problema  â†’  2. Definir ObservaÃ§Ãµes     â”‚
â”‚           â†“                            â†“                 â”‚
â”‚  7. Refinar e Otimizar  â†  3. Definir AÃ§Ãµes             â”‚
â”‚           â†‘                            â†“                 â”‚
â”‚  6. Avaliar Resultados  â†  4. Criar Recompensas         â”‚
â”‚           â†‘                            â†“                 â”‚
â”‚  5. Treinar o Modelo    â†  Definir Objetivos            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### ðŸ“‹ Passo 1: Entender o Problema de NegÃ³cio

**Objetivo**: Definir claramente o que vocÃª quer otimizar.

**Perguntas a responder:**
- Qual decisÃ£o vocÃª precisa automatizar?
- Quais informaÃ§Ãµes vocÃª tem disponÃ­veis?
- Quais aÃ§Ãµes sÃ£o possÃ­veis?
- O que define sucesso?

**Exemplo prÃ¡tico:**

```python
"""
PROBLEMA: Sistema de precificaÃ§Ã£o para e-commerce

DECISÃƒO: Que preÃ§o cobrar por produto a cada hora
INFORMAÃ‡Ã•ES: Estoque atual, demanda, preÃ§o concorrente, hora do dia
AÃ‡Ã•ES: Definir preÃ§o entre R$50 e R$500
SUCESSO: Maximizar receita mantendo competitividade
"""
```

---

### ðŸ”­ Passo 2: Definir o EspaÃ§o de ObservaÃ§Ã£o

**Objetivo**: Especificar todas as informaÃ§Ãµes que o agente pode "ver".

**Tipos de observaÃ§Ã£o:**

```python
import business_rl as brl

# 1. VALORES CONTÃNUOS (nÃºmeros decimais)
obs_continuo = brl.Box(low=0, high=100)  # Ex: preÃ§o, temperatura

# 2. VALORES DISCRETOS (categorias)
obs_discreto = brl.Discrete(5)  # Ex: 5 categorias diferentes

# 3. MÃšLTIPLAS OBSERVAÃ‡Ã•ES (dicionÃ¡rio)
obs_multiplo = brl.Dict(
    preco=brl.Box(0, 1000),        # Valor contÃ­nuo
    categoria=brl.Discrete(10),     # 10 categorias
    em_promocao=brl.Discrete(2)     # Sim/NÃ£o
)

# 4. OBSERVAÃ‡Ã•ES VETORIAIS (arrays)
obs_vetor = brl.Box(0, 1, shape=(10,))  # Array de 10 valores
```

**Exemplo completo:**

```python
@brl.problem(name="PrecificacaoDinamica")
class Precificacao:
    # O que o agente observa do ambiente
    obs = brl.Dict(
        # PreÃ§o atual do produto
        preco_atual=brl.Box(0, 1000),

        # Quantidade em estoque
        estoque=brl.Box(0, 500),

        # PreÃ§o do concorrente principal
        preco_concorrente=brl.Box(0, 1000),

        # Demanda nas Ãºltimas 24h
        demanda_24h=brl.Box(0, 1000),

        # Dia da semana (0=segunda, 6=domingo)
        dia_semana=brl.Discrete(7),

        # Hora do dia (0-23)
        hora=brl.Discrete(24),

        # EstÃ¡ em temporada alta?
        temporada_alta=brl.Discrete(2, labels=["nao", "sim"])
    )
```

**âš ï¸ Dicas importantes:**
- âœ… Inclua apenas informaÃ§Ãµes relevantes para a decisÃ£o
- âœ… Normalize valores grandes (divida por um mÃ¡ximo)
- âœ… Use labels descritivos para variÃ¡veis discretas
- âŒ Evite incluir informaÃ§Ãµes redundantes ou irrelevantes

---

### ðŸŽ® Passo 3: Definir o EspaÃ§o de AÃ§Ã£o

**Objetivo**: Especificar todas as aÃ§Ãµes que o agente pode tomar.

**Tipos de aÃ§Ã£o:**

```python
# 1. AÃ‡ÃƒO DISCRETA (escolher entre opÃ§Ãµes)
action = brl.Discrete(3, labels=["baixo", "medio", "alto"])

# 2. AÃ‡ÃƒO CONTÃNUA (valor numÃ©rico)
action = brl.Box(0, 1000)  # Ex: definir um preÃ§o

# 3. AÃ‡Ã•ES MÃšLTIPLAS
action = brl.Dict(
    preco=brl.Box(0, 1000),
    desconto=brl.Box(0, 0.5),  # 0% a 50%
    promocao=brl.Discrete(2, labels=["nao", "sim"])
)

# 4. AÃ‡ÃƒO HÃBRIDA (discreta + contÃ­nua)
action = brl.Mixed(
    discreto=brl.Dict(
        estrategia=brl.Discrete(3, labels=["agressiva", "moderada", "conservadora"])
    ),
    continuo=brl.Dict(
        preco=brl.Box(50, 500),
        duracao_dias=brl.Box(1, 30)
    )
)
```

**Exemplo completo:**

```python
@brl.problem(name="PrecificacaoDinamica")
class Precificacao:
    obs = brl.Dict(...)  # Definido no Passo 2

    # AÃ§Ã£o: ajustar preÃ§o e decidir sobre promoÃ§Ã£o
    action = brl.Dict(
        # Novo preÃ§o a cobrar
        preco=brl.Box(50, 500),

        # Percentual de desconto (0-30%)
        desconto=brl.Box(0, 0.30),

        # Destacar o produto?
        destaque=brl.Discrete(2, labels=["nao", "sim"])
    )
```

---

### ðŸŽ¯ Passo 4: Criar FunÃ§Ãµes de Recompensa

**Objetivo**: Ensinar ao agente o que Ã© "bom" ou "ruim".

**PrincÃ­pios das recompensas:**
- Deve ser **mensurÃ¡vel** (retornar um nÃºmero)
- Deve ser **frequente** (nÃ£o apenas no final)
- Deve refletir o **objetivo real**

**Template bÃ¡sico:**

```python
def reward_nome(self, state, action, next_state):
    """
    Args:
        state: Estado antes da aÃ§Ã£o
        action: AÃ§Ã£o tomada
        next_state: Estado depois da aÃ§Ã£o

    Returns:
        float: Valor da recompensa (maior = melhor)
    """
    # Seu cÃ¡lculo aqui
    return recompensa
```

**Exemplos prÃ¡ticos:**

```python
@brl.problem(name="PrecificacaoDinamica")
class Precificacao:
    obs = brl.Dict(...)
    action = brl.Dict(...)

    # Recompensa 1: Maximizar receita
    def reward_receita(self, state, action, next_state):
        """Calcula a receita gerada pela decisÃ£o de preÃ§o."""
        # Estima vendas baseado no preÃ§o e desconto
        preco_final = action['preco'] * (1 - action['desconto'])

        # Modelo simples: quanto mais barato, mais vende
        # (vocÃª pode usar dados reais aqui)
        elasticidade = 2.0  # Sensibilidade ao preÃ§o
        demanda_base = state['demanda_24h']
        ratio_preco = preco_final / state['preco_concorrente']

        vendas_estimadas = demanda_base * (ratio_preco ** -elasticidade)
        vendas_reais = min(vendas_estimadas, state['estoque'])

        receita = preco_final * vendas_reais
        return receita

    # Recompensa 2: Manter competitividade
    def reward_competitividade(self, state, action, next_state):
        """Penaliza se ficar muito mais caro que concorrente."""
        preco_final = action['preco'] * (1 - action['desconto'])
        diferenca = preco_final - state['preco_concorrente']

        if diferenca > 100:  # Muito mais caro
            return -10
        elif diferenca < -50:  # Muito mais barato (perde margem)
            return -5
        else:
            return 0  # PreÃ§o competitivo

    # Recompensa 3: Evitar estoque zero
    def reward_estoque(self, state, action, next_state):
        """Penaliza se estoque ficar muito baixo."""
        if next_state['estoque'] < 10:
            return -20  # Penalidade grande
        elif next_state['estoque'] < 50:
            return -5   # Penalidade pequena
        else:
            return 0
```

**âš ï¸ Armadilhas comuns:**

```python
# âŒ RUIM: Recompensa muito esparsa
def reward_ruim(self, state, action, next_state):
    # SÃ³ dÃ¡ recompensa no fim do mÃªs
    if next_state['dia'] == 30:
        return calcular_lucro_mensal()
    return 0  # Nada nos outros dias (agente nÃ£o aprende)

# âœ… BOM: Recompensa frequente
def reward_bom(self, state, action, next_state):
    # Recompensa a cada decisÃ£o
    return calcular_lucro_diario()

# âŒ RUIM: Recompensa nÃ£o reflete objetivo
def reward_ruim(self, state, action, next_state):
    # Objetivo: maximizar lucro
    # Recompensa: nÃºmero de vendas (ignora margem!)
    return action['vendas']

# âœ… BOM: Recompensa alinhada com objetivo
def reward_bom(self, state, action, next_state):
    receita = action['preco'] * action['vendas']
    custo = action['vendas'] * self.custo_unitario
    return receita - custo  # Lucro real
```

---

### ðŸŽ² Passo 5: Definir Objetivos e RestriÃ§Ãµes

**Objetivo**: Combinar mÃºltiplas recompensas e adicionar restriÃ§Ãµes.

#### 5.1 MÃºltiplos Objetivos

```python
@brl.problem(name="PrecificacaoDinamica")
class Precificacao:
    obs = brl.Dict(...)
    action = brl.Dict(...)

    # Combina mÃºltiplas recompensas com pesos
    objectives = brl.Terms(
        receita=0.5,              # 50% do peso
        competitividade=0.3,      # 30% do peso
        estoque=0.2               # 20% do peso
    )

    # As funÃ§Ãµes de recompensa devem ter os mesmos nomes
    def reward_receita(self, state, action, next_state):
        ...

    def reward_competitividade(self, state, action, next_state):
        ...

    def reward_estoque(self, state, action, next_state):
        ...
```

#### 5.2 Adicionar RestriÃ§Ãµes

```python
@brl.problem(name="PrecificacaoDinamica")
class Precificacao:
    obs = brl.Dict(...)
    action = brl.Dict(...)
    objectives = brl.Terms(...)

    # Define limites que o agente deve respeitar
    constraints = {
        # RestriÃ§Ã£o HARD: nunca pode violar
        'preco_minimo': brl.Limit(
            func=lambda s, a: a['preco'],
            min_val=50,   # PreÃ§o nÃ£o pode ser < R$50
            hard=True     # AÃ§Ã£o invÃ¡lida se violar
        ),

        # RestriÃ§Ã£o SOFT: pode violar mas recebe penalidade
        'margem_minima': brl.Limit(
            func=lambda s, a: a['preco'] - s['custo_unitario'],
            min_val=20,   # Margem mÃ­nima de R$20
            hard=False    # Pode violar mas Ã© penalizado
        ),

        # RestriÃ§Ã£o de intervalo
        'desconto_maximo': brl.Limit(
            func=lambda s, a: a['desconto'],
            max_val=0.30,  # MÃ¡ximo 30% de desconto
            hard=True
        )
    }
```

#### 5.3 GestÃ£o de Risco (Opcional)

```python
@brl.problem(name="PrecificacaoDinamica")
class Precificacao:
    obs = brl.Dict(...)
    action = brl.Dict(...)
    objectives = brl.Terms(...)
    constraints = {...}

    # Considera os piores cenÃ¡rios
    risk = brl.CVaR(
        alpha=0.05,         # Considera 5% piores resultados
        max_drawdown=0.2    # MÃ¡xima perda aceitÃ¡vel de 20%
    )
```

---

### ðŸ‹ï¸ Passo 6: Testar e Validar o Problema

**Objetivo**: Garantir que sua definiÃ§Ã£o estÃ¡ correta antes de treinar.

```python
import business_rl as brl

# 1. Crie o problema
problema = Precificacao()

# 2. Inspecione a definiÃ§Ã£o
print("=" * 50)
print("INFORMAÃ‡Ã•ES DO PROBLEMA")
print("=" * 50)
print(problema.get_info())

# 3. Teste com dados de exemplo
estado_teste = {
    'preco_atual': 200,
    'estoque': 100,
    'preco_concorrente': 180,
    'demanda_24h': 50,
    'dia_semana': 0,  # Segunda
    'hora': 14,       # 14h
    'temporada_alta': 0  # NÃ£o
}

# 4. Teste uma aÃ§Ã£o de exemplo
acao_teste = {
    'preco': 190,
    'desconto': 0.10,  # 10%
    'destaque': 1      # Sim
}

# 5. Calcule as recompensas manualmente
print("\n" + "=" * 50)
print("TESTE DE RECOMPENSAS")
print("=" * 50)

# Simula prÃ³ximo estado (normalmente vem do ambiente)
proximo_estado = estado_teste.copy()
proximo_estado['estoque'] = 90  # Vendeu 10 unidades

# Testa cada recompensa
r_receita = problema.reward_receita(estado_teste, acao_teste, proximo_estado)
r_comp = problema.reward_competitividade(estado_teste, acao_teste, proximo_estado)
r_est = problema.reward_estoque(estado_teste, acao_teste, proximo_estado)

print(f"Receita: {r_receita:.2f}")
print(f"Competitividade: {r_comp:.2f}")
print(f"Estoque: {r_est:.2f}")

# 6. Teste com treino rÃ¡pido (6 minutos)
print("\n" + "=" * 50)
print("TESTE DE TREINO RÃPIDO")
print("=" * 50)

modelo = brl.train(problema, hours=0.1)  # 6 minutos
decisao = modelo.decide(estado_teste)

print(f"\nDecisÃ£o do modelo:")
print(f"  PreÃ§o: R$ {decisao.action['preco']:.2f}")
print(f"  Desconto: {decisao.action['desconto']*100:.1f}%")
print(f"  Destaque: {decisao.action['destaque']}")
print(f"  ConfianÃ§a: {decisao.confidence:.2%}")
```

---

### ðŸš‚ Passo 7: Treinar o Modelo

**Objetivo**: Treinar o agente com configuraÃ§Ãµes adequadas.

#### 7.1 Treino BÃ¡sico

```python
import business_rl as brl

problema = Precificacao()

# Treino simples (usa configuraÃ§Ãµes padrÃ£o)
modelo = brl.train(problema, hours=1)

# Salvar o modelo
modelo.save('./modelos/precificacao_v1.pt')
```

#### 7.2 Treino AvanÃ§ado

```python
# Mais controle sobre o processo
modelo = brl.train(
    problema,
    algorithm='PPO',      # Algoritmo (PPO ou SAC)
    hours=2,              # Tempo de treino
    config={
        'learning_rate': 3e-4,     # Taxa de aprendizado
        'batch_size': 256,         # Tamanho do lote
        'n_epochs': 10,            # Ã‰pocas por atualizaÃ§Ã£o
        'gamma': 0.99,             # Fator de desconto
        'gae_lambda': 0.95,        # GAE para vantagem
        'clip_range': 0.2,         # Clipping PPO
        'ent_coef': 0.01,          # Coeficiente de entropia
        'vf_coef': 0.5,            # Coeficiente de value function
    }
)
```

#### 7.3 Treino com Dashboard

```python
from business_rl.tools import TrainingDashboard

# Cria trainer
trainer = brl.Trainer(problema, algorithm='PPO')

# Inicia dashboard (abra http://localhost:5000 no navegador)
dashboard = TrainingDashboard(trainer, port=5000)
dashboard.start()

# Treina monitorando em tempo real
modelo = trainer.train(
    episodes=10000,           # NÃºmero de episÃ³dios
    save_path='./modelos/precificacao_v1.pt',
    checkpoint_freq=1000      # Salva a cada 1000 episÃ³dios
)
```

#### 7.4 Escolhendo o Algoritmo

```python
# PPO (Proximal Policy Optimization) - RECOMENDADO PARA INICIANTES
# âœ… Mais estÃ¡vel
# âœ… Funciona bem em vÃ¡rios problemas
# âœ… Bom para espaÃ§os discretos e contÃ­nuos
modelo_ppo = brl.train(problema, algorithm='PPO', hours=1)

# SAC (Soft Actor-Critic) - PARA AÃ‡Ã•ES CONTÃNUAS
# âœ… Melhor para aÃ§Ãµes contÃ­nuas complexas
# âœ… Mais exploraÃ§Ã£o
# âš ï¸ Pode ser mais lento
modelo_sac = brl.train(problema, algorithm='SAC', hours=2)
```

---

### ðŸ“Š Passo 8: Avaliar e Refinar

**Objetivo**: Validar o desempenho e iterar para melhorar.

#### 8.1 Teste com Dados Reais

```python
# Carrega modelo treinado
modelo = brl.load('./modelos/precificacao_v1.pt')

# Testa com mÃºltiplos cenÃ¡rios
cenarios = [
    {
        'nome': 'Alta demanda',
        'estado': {'preco_atual': 200, 'estoque': 100, 'demanda_24h': 200, ...}
    },
    {
        'nome': 'Baixa demanda',
        'estado': {'preco_atual': 200, 'estoque': 100, 'demanda_24h': 20, ...}
    },
    {
        'nome': 'Estoque baixo',
        'estado': {'preco_atual': 200, 'estoque': 10, 'demanda_24h': 100, ...}
    }
]

print("=" * 60)
print("AVALIAÃ‡ÃƒO DO MODELO")
print("=" * 60)

for cenario in cenarios:
    decisao = modelo.decide(cenario['estado'], deterministic=True)

    print(f"\n{cenario['nome']}:")
    print(f"  PreÃ§o: R$ {decisao.action['preco']:.2f}")
    print(f"  Desconto: {decisao.action['desconto']*100:.1f}%")
    print(f"  ConfianÃ§a: {decisao.confidence:.2%}")
```

#### 8.2 Comparar com Baseline

```python
# Crie uma polÃ­tica simples para comparaÃ§Ã£o
def politica_simples(estado):
    """Sempre cobra 10% a menos que o concorrente."""
    return {
        'preco': estado['preco_concorrente'] * 0.9,
        'desconto': 0.0,
        'destaque': 0
    }

# Compare
estados_teste = [...]  # Seus dados de teste

receita_modelo = 0
receita_baseline = 0

for estado in estados_teste:
    # DecisÃ£o do modelo
    decisao_modelo = modelo.decide(estado)
    receita_modelo += simular_receita(estado, decisao_modelo.action)

    # DecisÃ£o baseline
    decisao_baseline = politica_simples(estado)
    receita_baseline += simular_receita(estado, decisao_baseline)

print(f"\nReceita Total:")
print(f"  Modelo RL: R$ {receita_modelo:,.2f}")
print(f"  Baseline:  R$ {receita_baseline:,.2f}")
print(f"  Melhoria:  {(receita_modelo/receita_baseline - 1)*100:.1f}%")
```

#### 8.3 Identificar Problemas Comuns

```python
# Problema 1: Modelo nÃ£o aprende
# SoluÃ§Ã£o: Verifique as recompensas
print("Recompensas mÃ©dias por episÃ³dio:")
# Se sempre prÃ³ximo de zero -> recompensas mal definidas

# Problema 2: AÃ§Ãµes sempre iguais
# SoluÃ§Ã£o: Aumente exploraÃ§Ã£o
modelo = brl.train(problema, hours=1, config={
    'ent_coef': 0.1  # Aumenta entropia (exploraÃ§Ã£o)
})

# Problema 3: Desempenho instÃ¡vel
# SoluÃ§Ã£o: Reduza learning rate
modelo = brl.train(problema, hours=2, config={
    'learning_rate': 1e-4  # Menor que o padrÃ£o (3e-4)
})

# Problema 4: Viola restriÃ§Ãµes
# SoluÃ§Ã£o: Torne restriÃ§Ãµes HARD
constraints = {
    'preco_minimo': brl.Limit(..., hard=True)  # Era False
}
```

#### 8.4 Iterar e Melhorar

```python
# VERSÃƒO 1: Modelo bÃ¡sico
modelo_v1 = brl.train(problema_v1, hours=1)
# Resultado: 70% de acurÃ¡cia

# VERSÃƒO 2: Adiciona mais observaÃ§Ãµes
problema_v2.obs = brl.Dict(
    # ... obs anteriores ...
    historico_vendas=brl.Box(0, 1000, shape=(7,))  # Ãšltimos 7 dias
)
modelo_v2 = brl.train(problema_v2, hours=1.5)
# Resultado: 78% de acurÃ¡cia

# VERSÃƒO 3: Refina recompensas
def reward_receita_v3(self, state, action, next_state):
    # VersÃ£o melhorada com modelo de demanda mais realista
    ...

modelo_v3 = brl.train(problema_v3, hours=2)
# Resultado: 85% de acurÃ¡cia

# VERSÃƒO 4: Treina por mais tempo
modelo_v4 = brl.train(problema_v3, hours=5)
# Resultado: 90% de acurÃ¡cia
```

---

### âœ… Checklist de Desenvolvimento

Use este checklist ao desenvolver seu modelo:

#### Fase 1: DefiniÃ§Ã£o
- [ ] Problema de negÃ³cio estÃ¡ claro
- [ ] ObservaÃ§Ãµes incluem todas as informaÃ§Ãµes relevantes
- [ ] AÃ§Ãµes representam todas as decisÃµes possÃ­veis
- [ ] FunÃ§Ãµes de recompensa refletem os objetivos reais

#### Fase 2: ValidaÃ§Ã£o
- [ ] `problema.get_info()` mostra informaÃ§Ãµes corretas
- [ ] Testei recompensas manualmente com dados de exemplo
- [ ] Treino rÃ¡pido (6 min) nÃ£o dÃ¡ erros
- [ ] RestriÃ§Ãµes estÃ£o bem definidas

#### Fase 3: Treino
- [ ] Escolhi o algoritmo apropriado (PPO ou SAC)
- [ ] Defini tempo de treino adequado (1-3h inicial)
- [ ] Configurei dashboard para monitoramento
- [ ] Salvei checkpoints durante o treino

#### Fase 4: AvaliaÃ§Ã£o
- [ ] Testei com dados reais/realistas
- [ ] Comparei com baseline simples
- [ ] Modelo performa melhor que baseline
- [ ] DecisÃµes fazem sentido intuitivamente

#### Fase 5: ProduÃ§Ã£o
- [ ] Documentei versÃ£o e data do modelo
- [ ] Salvei configuraÃ§Ãµes de treino
- [ ] Defini processo de re-treino
- [ ] Criei monitoramento de desempenho

---

## ðŸ”§ InstalaÃ§Ã£o

### OpÃ§Ã£o 1: InstalaÃ§Ã£o Local (Desenvolvimento)
```bash
cd C:\Users\incorporacao-04\Desktop\AGENTS_RL_AVANÃ‡ADOS\RL_001
pip install -e .
```

### OpÃ§Ã£o 2: Instalar dependÃªncias manualmente
```bash
pip install torch numpy gym flask
```

---

## ðŸŒŸ Primeiro Exemplo (BÃ¡sico)

### Problema Simples: DecisÃ£o de Compra

```python
import business_rl as brl

# 1. Defina seu problema
@brl.problem(name="DecisaoCompra")
class DecisaoCompra:
    """Decidir se compra ou nÃ£o baseado em preÃ§o e qualidade."""
    
    # O que vocÃª observa do ambiente
    obs = brl.Dict(
        preco=brl.Box(0, 1000),      # PreÃ§o do produto
        qualidade=brl.Box(0, 10)      # Qualidade (0-10)
    )
    
    # AÃ§Ãµes disponÃ­veis
    action = brl.Discrete(2, labels=["nao_comprar", "comprar"])
    
    # O que vocÃª quer maximizar
    objectives = brl.Terms(
        lucro=1.0  # Maximizar lucro
    )
    
    # FunÃ§Ã£o de recompensa
    def reward_lucro(self, state, action, next_state):
        """Calcula o lucro da decisÃ£o."""
        if action == 1:  # comprou
            # Lucro = qualidade - custo
            return next_state['qualidade'] * 10 - next_state['preco']
        return 0  # nÃ£o comprou

# 2. Crie o problema
problema = DecisaoCompra()

# 3. Treine o modelo (1 hora)
modelo = brl.train(problema, hours=1)

# 4. Use para tomar decisÃµes
decisao = modelo.decide({
    "preco": 500,
    "qualidade": 8
})

print(f"DecisÃ£o: {decisao.action}")
print(f"ConfianÃ§a: {decisao.confidence}")
```

---

## ðŸŽ¯ Exemplo IntermediÃ¡rio

### Problema: OtimizaÃ§Ã£o de Campanha de Ads

```python
import business_rl as brl

@brl.problem(name="CampanhaAds")
class CampanhaAds:
    """Otimizar budget de campanha entre canais."""
    
    # Estado: mÃ©tricas atuais
    obs = brl.Dict(
        budget_disponivel=brl.Box(0, 10000),
        ctr_facebook=brl.Box(0, 1),
        ctr_google=brl.Box(0, 1),
        conversoes_mes=brl.Box(0, 1000)
    )
    
    # AÃ§Ã£o: quanto alocar para cada canal
    action = brl.Dict(
        facebook=brl.Box(0, 1),      # % do budget
        google=brl.Box(0, 1),        # % do budget
        instagram=brl.Box(0, 1)      # % do budget
    )
    
    # MÃºltiplos objetivos
    objectives = brl.Terms(
        roi=0.7,              # 70% peso no ROI
        conversoes=0.3        # 30% peso em conversÃµes
    )
    
    # RestriÃ§Ãµes
    constraints = {
        'budget_total': brl.Limit(
            lambda s, a: a['facebook'] + a['google'] + a['instagram'],
            max_val=1.0,  # Soma nÃ£o pode passar de 100%
            hard=True
        )
    }
    
    # GestÃ£o de risco
    risk = brl.CVaR(
        alpha=0.05,           # 5% piores casos
        max_drawdown=0.2      # MÃ¡ximo 20% de perda
    )
    
    def reward_roi(self, state, action, next_state):
        """ROI da alocaÃ§Ã£o."""
        budget = state['budget_disponivel']
        gasto_fb = budget * action['facebook']
        gasto_gg = budget * action['google']
        gasto_ig = budget * action['instagram']
        
        # Simula receita (vocÃª substituiria por dados reais)
        receita = (gasto_fb * state['ctr_facebook'] * 50 +
                   gasto_gg * state['ctr_google'] * 45 +
                   gasto_ig * 0.03 * 40)
        
        gasto_total = gasto_fb + gasto_gg + gasto_ig
        return (receita - gasto_total) / (gasto_total + 1e-6)
    
    def reward_conversoes(self, state, action, next_state):
        """NÃºmero de conversÃµes estimadas."""
        budget = state['budget_disponivel']
        return (budget * action['facebook'] * state['ctr_facebook'] * 0.02 +
                budget * action['google'] * state['ctr_google'] * 0.015)

# Treinar com configuraÃ§Ã£o avanÃ§ada
problema = CampanhaAds()

modelo = brl.train(
    problema,
    algorithm='PPO',          # Algoritmo
    hours=2,                  # 2 horas de treino
    config={
        'learning_rate': 3e-4,
        'batch_size': 256,
        'n_epochs': 10
    }
)

# Usar
estado_atual = {
    'budget_disponivel': 5000,
    'ctr_facebook': 0.05,
    'ctr_google': 0.08,
    'conversoes_mes': 150
}

decisao = modelo.decide(estado_atual)
print(f"AlocaÃ§Ã£o recomendada:")
print(f"  Facebook: {decisao.action['facebook']*100:.1f}%")
print(f"  Google: {decisao.action['google']*100:.1f}%")
print(f"  Instagram: {decisao.action['instagram']*100:.1f}%")
```

---

## ðŸ¢ Exemplo AvanÃ§ado: Compra de Terreno

### Usando o Problema PrÃ©-ConstruÃ­do

```python
import business_rl as brl
from business_rl.domains.real_estate import CompraTerreno

# 1. Usa problema prÃ©-construÃ­do
problema = CompraTerreno()

# 2. Treina com dashboard em tempo real
trainer = brl.Trainer(problema, algorithm='PPO')

# Abre dashboard no navegador
dashboard = brl.TrainingDashboard(trainer, port=5000)
dashboard.start()

# Treina (acompanhe no navegador em http://localhost:5000)
modelo = trainer.train(hours=3)

# 3. Avalia um terreno especÃ­fico
terreno = {
    'preco_m2': 500,
    'area_total': 1000,
    'zoneamento': 'residencial',
    'acesso_agua': True,
    'acesso_luz': True,
    'distancia_centro': 5.0,
    'valorizacao_historica': 0.08
}

decisao = modelo.decide(terreno)
print(f"DecisÃ£o: {decisao.action}")
print(f"Valor estimado: ${decisao.value:.2f}")
print(f"ConfianÃ§a: {decisao.confidence:.2%}")
```

---

## ðŸ“¦ Problemas PrÃ©-ConstruÃ­dos

### 1. Compra de Terreno

```python
from business_rl.domains.real_estate import CompraTerreno

problema = CompraTerreno()
modelo = brl.train(problema, hours=1)

decisao = modelo.decide({
    'preco_m2': 500,
    'area_total': 1000,
    'zoneamento': 'residencial',
    # ... outros campos
})
```

### 2. Campanha de Ads

```python
from business_rl.domains.marketing import CampanhaAds

problema = CampanhaAds()
modelo = brl.train(problema, hours=1)

decisao = modelo.decide({
    'budget_disponivel': 5000,
    'ctr_facebook': 0.05,
    # ... outros campos
})
```

---

## ðŸ” API Completa

### 1. Definir ObservaÃ§Ãµes

```python
# ObservaÃ§Ã£o simples (nÃºmero)
obs = brl.Box(0, 100)

# ObservaÃ§Ãµes mÃºltiplas (dicionÃ¡rio)
obs = brl.Dict(
    preco=brl.Box(0, 1000),
    quantidade=brl.Box(0, 100),
    categoria=brl.Discrete(5)  # 5 categorias
)
```

### 2. Definir AÃ§Ãµes

```python
# AÃ§Ã£o discreta simples
action = brl.Discrete(3, labels=["baixo", "medio", "alto"])

# AÃ§Ã£o contÃ­nua
action = brl.Box(0, 1)

# AÃ§Ãµes mÃºltiplas
action = brl.Dict(
    preco=brl.Box(0, 1000),
    promocao=brl.Discrete(2, labels=["sim", "nao"])
)

# AÃ§Ãµes hÃ­bridas (discretas + contÃ­nuas)
action = brl.Mixed(
    discreto=brl.Dict(
        tipo_campanha=brl.Discrete(3, labels=["agressiva", "moderada", "conservadora"])
    ),
    continuo=brl.Dict(
        budget=brl.Box(0, 10000),
        duracao_dias=brl.Box(1, 30)
    )
)
```

### 3. Definir Objetivos

```python
# Objetivo simples
objectives = brl.Terms(lucro=1.0)

# MÃºltiplos objetivos com pesos
objectives = brl.Terms(
    lucro=0.6,
    satisfacao_cliente=0.3,
    impacto_ambiental=0.1
)
```

### 4. Definir RestriÃ§Ãµes

```python
constraints = {
    'budget': brl.Limit(
        func=lambda s, a: a['gasto_total'],
        max_val=10000,
        hard=True  # Nunca pode violar
    ),
    'tempo': brl.Limit(
        func=lambda s, a: a['horas_trabalho'],
        min_val=8,
        max_val=40,
        hard=False  # Pode violar com penalidade
    )
}
```

### 5. GestÃ£o de Risco

```python
risk = brl.CVaR(
    alpha=0.05,           # Considera 5% piores cenÃ¡rios
    max_drawdown=0.2      # MÃ¡ximo 20% de perda aceitÃ¡vel
)
```

### 6. Treinar

```python
# BÃ¡sico
modelo = brl.train(problema, hours=1)

# AvanÃ§ado
modelo = brl.train(
    problema,
    algorithm='PPO',  # ou 'SAC'
    hours=2,
    config={
        'learning_rate': 3e-4,
        'batch_size': 256,
        'gamma': 0.99,
        'n_epochs': 10
    }
)

# Com Trainer (mais controle)
trainer = brl.Trainer(problema, algorithm='PPO')
modelo = trainer.train(
    episodes=1000,
    save_path='./modelos/meu_modelo.pt'
)
```

### 7. Usar Modelo

```python
# DecisÃ£o determinÃ­stica
decisao = modelo.decide(estado, deterministic=True)

# DecisÃ£o com exploraÃ§Ã£o
decisao = modelo.decide(estado, deterministic=False)

# Acessar informaÃ§Ãµes da decisÃ£o
print(decisao.action)       # AÃ§Ã£o escolhida
print(decisao.value)        # Valor esperado
print(decisao.confidence)   # ConfianÃ§a (0-1)
print(decisao.log_prob)     # Log-probabilidade
print(decisao.entropy)      # Entropia (exploraÃ§Ã£o)
```

---

## ðŸ’¡ Dicas e Boas PrÃ¡ticas

### 1. ComeÃ§ar Simples
```python
# âœ… BOM: Comece com problema simples
@brl.problem(name="Simples")
class Simples:
    obs = brl.Box(0, 100)
    action = brl.Discrete(2)
    objectives = brl.Terms(lucro=1.0)

# âŒ EVITE: ComeÃ§ar com muita complexidade
```

### 2. Normalizar ObservaÃ§Ãµes
```python
# âœ… BOM: Valores normalizados (0-1)
obs = brl.Dict(
    preco_normalizado=brl.Box(0, 1),  # Dividiu por max
    quantidade_normalizada=brl.Box(0, 1)
)

# âŒ EVITE: Escalas muito diferentes
obs = brl.Dict(
    preco=brl.Box(0, 1000000),  # Muito grande
    quantidade=brl.Box(0, 10)    # Muito pequena
)
```

### 3. FunÃ§Ã£o de Recompensa Clara
```python
# âœ… BOM: Recompensa clara e mensurÃ¡vel
def reward_lucro(self, state, action, next_state):
    receita = action['preco'] * action['quantidade']
    custo = action['quantidade'] * 10
    return receita - custo

# âŒ EVITE: Recompensa complexa demais
def reward_lucro(self, state, action, next_state):
    # Muitas condiÃ§Ãµes, difÃ­cil de aprender
    if state['dia'] == 'segunda' and action > 5:
        if next_state['estoque'] < 100:
            return math.log(action) * state['preco'] ** 2
    # ...
```

### 4. Testar Incrementalmente
```python
# 1. Teste o problema
problema = MeuProblema()
print(problema.get_info())

# 2. Teste com poucos episÃ³dios
modelo = brl.train(problema, hours=0.1)  # 6 minutos

# 3. Teste decisÃ£o
decisao = modelo.decide(estado_teste)
print(decisao)

# 4. Se funcionar, aumente o treino
modelo = brl.train(problema, hours=1)
```

### 5. Usar Dashboard
```python
# Monitore o treino em tempo real
from business_rl.tools import TrainingDashboard

trainer = brl.Trainer(problema)
dashboard = TrainingDashboard(trainer, port=5000)
dashboard.start()

# Abra http://localhost:5000 no navegador
modelo = trainer.train(hours=2)
```

---

## ðŸŽ“ Exemplos de Casos de Uso

### E-commerce: PrecificaÃ§Ã£o DinÃ¢mica
```python
@brl.problem(name="PrecificacaoDinamica")
class Precificacao:
    obs = brl.Dict(
        demanda_atual=brl.Box(0, 1000),
        estoque=brl.Box(0, 500),
        preco_concorrente=brl.Box(0, 200),
        dia_semana=brl.Discrete(7)
    )
    
    action = brl.Box(0, 200)  # PreÃ§o a cobrar
    
    objectives = brl.Terms(
        receita=0.8,
        market_share=0.2
    )
```

### LogÃ­stica: Roteamento de Entregas
```python
@brl.problem(name="Roteamento")
class Roteamento:
    obs = brl.Dict(
        localizacao_atual=brl.Box(-180, 180, shape=(2,)),  # lat, lon
        entregas_pendentes=brl.Box(0, 50),
        trafego=brl.Box(0, 1),
        combustivel=brl.Box(0, 100)
    )
    
    action = brl.Discrete(10)  # PrÃ³xima entrega (top 10)
    
    objectives = brl.Terms(
        tempo=0.5,
        custo=0.3,
        satisfacao=0.2
    )
```

### FinanÃ§as: Portfolio Management
```python
@brl.problem(name="Portfolio")
class Portfolio:
    obs = brl.Dict(
        precos_acoes=brl.Box(0, 1000, shape=(10,)),  # 10 aÃ§Ãµes
        portfolio_atual=brl.Box(0, 1, shape=(10,)),   # % alocado
        volatilidade=brl.Box(0, 1, shape=(10,))
    )
    
    action = brl.Box(0, 1, shape=(10,))  # Nova alocaÃ§Ã£o
    
    objectives = brl.Terms(retorno=0.7, risco=0.3)
    
    risk = brl.CVaR(alpha=0.05, max_drawdown=0.15)
```

---

## ðŸ“ž Precisa de Ajuda?

- ðŸ“š **DocumentaÃ§Ã£o**: Veja exemplos em `business_rl/examples/`
- ðŸ› **Problemas**: Verifique `log.txt`
- ðŸ’¡ **DÃºvidas**: Consulte este guia

---

## ðŸš€ PrÃ³ximos Passos

1. âœ… Execute `executar_validacao.bat` para garantir que tudo estÃ¡ funcionando
2. ðŸ“ Copie um dos exemplos acima
3. âœï¸ Adapte para seu problema
4. ðŸƒ Execute e experimente!

**Boa sorte! ðŸŽ‰**
