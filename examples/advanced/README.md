# üéì Exemplos Avan√ßados - Business RL

Esta pasta cont√©m exemplos avan√ßados e completos de uso do framework Business-RL.

## üìã Exemplos Dispon√≠veis

### 1. üíº Gest√£o de Portf√≥lio (`01_portfolio_management.py`)

**Problema:** Otimizar aloca√ß√£o de capital em 5 ativos diferentes

**Demonstra:**
- ‚úÖ M√∫ltiplas observa√ß√µes (pre√ßos, volatilidade, retornos hist√≥ricos)
- ‚úÖ A√ß√µes cont√≠nuas (percentual de aloca√ß√£o)
- ‚úÖ M√∫ltiplos objetivos (retorno vs risco)
- ‚úÖ Restri√ß√µes complexas (limites de aloca√ß√£o)
- ‚úÖ Gest√£o de risco com CVaR
- ‚úÖ Algoritmo SAC para a√ß√µes cont√≠nuas

**Executar:**
```bash
python examples/advanced/01_portfolio_management.py
```

**O que voc√™ vai aprender:**
- Como modelar problemas financeiros
- Usar CVaR para gest√£o de risco
- Trabalhar com espa√ßos de a√ß√£o cont√≠nuos
- Comparar com estrat√©gias baseline

---

### 2. üí∞ Precifica√ß√£o Din√¢mica (`02_dynamic_pricing.py`)

**Problema:** Definir pre√ßo √≥timo para maximizar lucro em e-commerce

**Demonstra:**
- ‚úÖ Observa√ß√µes temporais (hora, dia, sazonalidade)
- ‚úÖ A√ß√µes h√≠bridas (pre√ßo + desconto + promo√ß√µes)
- ‚úÖ Modelagem de elasticidade de demanda
- ‚úÖ Competi√ß√£o com concorrentes
- ‚úÖ Trade-off entre margem e volume

**Executar:**
```bash
python examples/advanced/02_dynamic_pricing.py
```

**O que voc√™ vai aprender:**
- Modelar demanda el√°stica
- Combinar a√ß√µes discretas e cont√≠nuas
- Considerar sazonalidade
- Otimizar m√∫ltiplos objetivos conflitantes

---

### 3. üì¶ Gest√£o de Estoque (`03_inventory_management.py`)

**Problema:** Gerenciar estoque de m√∫ltiplos produtos

**Demonstra:**
- ‚úÖ M√∫ltiplos produtos simult√¢neos
- ‚úÖ Restri√ß√µes de or√ßamento e capacidade
- ‚úÖ Previs√£o de demanda
- ‚úÖ Lead time de fornecedores
- ‚úÖ Minimiza√ß√£o de rupturas e excessos

**Executar:**
```bash
python examples/advanced/03_inventory_management.py
```

**O que voc√™ vai aprender:**
- Gerenciar m√∫ltiplos produtos
- Trabalhar com restri√ß√µes din√¢micas
- Otimizar capital de giro
- Evitar obsolesc√™ncia

---

## üöÄ Como Executar os Exemplos

### Pr√©-requisitos

1. **Instalar o Business-RL:**
```bash
pip install git+https://github.com/cbaracho200/Reinforcement_Learning_version_000.git
```

2. **Ou instalar localmente:**
```bash
git clone https://github.com/cbaracho200/Reinforcement_Learning_version_000.git
cd Reinforcement_Learning_version_000
pip install -e .
```

### Executar um exemplo

```bash
# Navegar at√© a pasta do projeto
cd Reinforcement_Learning_version_000

# Executar exemplo espec√≠fico
python examples/advanced/01_portfolio_management.py
python examples/advanced/02_dynamic_pricing.py
python examples/advanced/03_inventory_management.py
```

### Ajustar tempo de treino

Por padr√£o, os exemplos treinam por 2 horas. Para testes r√°pidos, edite o arquivo e mude:

```python
# De:
modelo = brl.train(problema, hours=2)

# Para (teste r√°pido - 10 minutos):
modelo = brl.train(problema, hours=0.17)
```

---

## üìä Estrutura dos Exemplos

Todos os exemplos seguem a mesma estrutura:

```python
# 1. Defini√ß√£o do problema
@brl.problem(name="NomeProblema")
class MeuProblema:
    obs = brl.Dict(...)      # Observa√ß√µes
    action = brl.Dict(...)   # A√ß√µes
    objectives = brl.Terms(...)  # Objetivos
    constraints = {...}      # Restri√ß√µes
    risk = brl.CVaR(...)    # Gest√£o de risco (opcional)

    # Fun√ß√µes de recompensa
    def reward_objetivo1(self, state, action, next_state):
        ...

    def reward_objetivo2(self, state, action, next_state):
        ...

# 2. Fun√ß√£o de treino
def treinar_modelo():
    problema = MeuProblema()
    modelo = brl.train(problema, algorithm='PPO', hours=2)
    modelo.save('./modelos/meu_modelo.pt')
    return modelo

# 3. Fun√ß√£o de teste
def testar_modelo():
    modelo = brl.load('./modelos/meu_modelo.pt')
    # Testa com v√°rios cen√°rios
    ...

# 4. Execu√ß√£o
if __name__ == "__main__":
    modelo = treinar_modelo()
    testar_modelo()
```

---

## üéØ Conceitos Demonstrados

### 1. Tipos de Observa√ß√£o
- **Cont√≠nuas:** `brl.Box(0, 100)` - valores num√©ricos
- **Discretas:** `brl.Discrete(5)` - categorias
- **Vetoriais:** `brl.Box(0, 1, shape=(10,))` - arrays
- **Dicion√°rios:** `brl.Dict(...)` - m√∫ltiplas observa√ß√µes

### 2. Tipos de A√ß√£o
- **Discretas:** escolher entre op√ß√µes
- **Cont√≠nuas:** valores num√©ricos
- **H√≠bridas:** combina√ß√£o de discretas e cont√≠nuas

### 3. Restri√ß√µes
- **Hard:** nunca pode violar (a√ß√£o inv√°lida)
- **Soft:** pode violar com penalidade
- **Din√¢micas:** dependem do estado

### 4. Algoritmos
- **PPO:** recomendado para iniciantes, est√°vel
- **SAC:** melhor para a√ß√µes cont√≠nuas complexas

### 5. Gest√£o de Risco
- **CVaR:** considera piores cen√°rios
- **Max Drawdown:** limita perdas m√°ximas

---

## üí° Dicas para Usar os Exemplos

### 1. Come√ßar Simples
```bash
# Execute primeiro o exemplo mais simples
python examples/advanced/01_portfolio_management.py
```

### 2. Experimentar com Par√¢metros
```python
# Teste diferentes configura√ß√µes
modelo = brl.train(
    problema,
    algorithm='PPO',
    hours=1,  # Reduza para testes
    config={
        'learning_rate': 3e-4,  # Ajuste conforme necess√°rio
        'batch_size': 128,      # Reduza se tiver pouca mem√≥ria
    }
)
```

### 3. Criar Seus Pr√≥prios Cen√°rios
```python
# Adicione seus pr√≥prios casos de teste
cenarios = [
    {
        'nome': 'Meu Cen√°rio',
        'estado': {
            # Seus dados aqui
        }
    }
]
```

### 4. Comparar com Baseline
```python
# Sempre compare com uma estrat√©gia simples
def estrategia_simples(estado):
    return acao_padrao

# Compare resultados
resultado_rl = testar_modelo_rl()
resultado_baseline = testar_baseline()
print(f"Melhoria: {(resultado_rl/resultado_baseline - 1)*100:.1f}%")
```

---

## üîß Troubleshooting

### Erro: "M√≥dulo business_rl n√£o encontrado"
```bash
# Certifique-se de ter instalado
pip install git+https://github.com/cbaracho200/Reinforcement_Learning_version_000.git
```

### Treino muito lento
```python
# Reduza o tempo de treino
modelo = brl.train(problema, hours=0.5)  # 30 minutos

# Ou reduza batch_size
config={'batch_size': 64}
```

### Modelo n√£o aprende
```python
# 1. Verifique as recompensas
print(problema.reward_objetivo1(estado, acao, proximo_estado))

# 2. Aumente explora√ß√£o
config={'ent_coef': 0.1}

# 3. Reduza learning rate
config={'learning_rate': 1e-4}
```

---

## üìö Pr√≥ximos Passos

1. ‚úÖ Execute todos os exemplos
2. üìù Modifique um exemplo para seu caso de uso
3. üéØ Crie seu pr√≥prio problema do zero
4. üöÄ Compartilhe seus resultados!

---

## ü§ù Contribuindo

Tem um exemplo interessante? Contribua!

1. Fork o reposit√≥rio
2. Crie seu exemplo em `examples/advanced/`
3. Siga a estrutura dos exemplos existentes
4. Envie um Pull Request

---

## üìû Precisa de Ajuda?

- üìö Veja a documenta√ß√£o principal no `README.md`
- üí° Consulte a se√ß√£o "Como Desenvolver Modelos Passo a Passo"
- üêõ Reporte problemas no GitHub Issues

**Boa sorte! üéâ**
