# üéì Exemplos Avan√ßados - Business RL

Esta pasta cont√©m exemplos avan√ßados e completos de uso do framework Business-RL.

## üìã Exemplos Dispon√≠veis

### 1. üíº Gest√£o de Portf√≥lio (`01_portfolio_management.py`)

**Problema:** Otimizar aloca√ß√£o de capital em 5 ativos diferentes

**Demonstra:**
- ‚úÖ M√∫ltiplas observa√ß√µes (pre√ßos, volatilidade, retornos hist√≥ricos)
- ‚úÖ A√ß√µes cont√≠nuas (percentual de aloca√ß√£o)
- ‚úÖ M√∫ltiplos objetivos (retorno vs risco)
- ‚úÖ Restri√ß√µes complexas (limites de aloca√ß√£o)
- ‚úÖ Gest√£o de risco com CVaR
- ‚úÖ Algoritmo SAC para a√ß√µes cont√≠nuas

**Executar:**
```bash
python examples/advanced/01_portfolio_management.py
```

**O que voc√™ vai aprender:**
- Como modelar problemas financeiros
- Usar CVaR para gest√£o de risco
- Trabalhar com espa√ßos de a√ß√£o cont√≠nuos
- Comparar com estrat√©gias baseline

---

### 2. üí∞ Precifica√ß√£o Din√¢mica (`02_dynamic_pricing.py`)

**Problema:** Definir pre√ßo √≥timo para maximizar lucro em e-commerce

**Demonstra:**
- ‚úÖ Observa√ß√µes temporais (hora, dia, sazonalidade)
- ‚úÖ A√ß√µes h√≠bridas (pre√ßo + desconto + promo√ß√µes)
- ‚úÖ Modelagem de elasticidade de demanda
- ‚úÖ Competi√ß√£o com concorrentes
- ‚úÖ Trade-off entre margem e volume

**Executar:**
```bash
python examples/advanced/02_dynamic_pricing.py
```

**O que voc√™ vai aprender:**
- Modelar demanda el√°stica
- Combinar a√ß√µes discretas e cont√≠nuas
- Considerar sazonalidade
- Otimizar m√∫ltiplos objetivos conflitantes

---

### 3. üì¶ Gest√£o de Estoque (`03_inventory_management.py`)

**Problema:** Gerenciar estoque de m√∫ltiplos produtos

**Demonstra:**
- ‚úÖ M√∫ltiplos produtos simult√¢neos
- ‚úÖ Restri√ß√µes de or√ßamento e capacidade
- ‚úÖ Previs√£o de demanda
- ‚úÖ Lead time de fornecedores
- ‚úÖ Minimiza√ß√£o de rupturas e excessos

**Executar:**
```bash
python examples/advanced/03_inventory_management.py
```

**O que voc√™ vai aprender:**
- Gerenciar m√∫ltiplos produtos
- Trabalhar com restri√ß√µes din√¢micas
- Otimizar capital de giro
- Evitar obsolesc√™ncia

---

### 4. üéØ Contextual Bandits (`04_contextual_bandits_personalization.py`)

**Problema:** Personaliza√ß√£o de conte√∫do em tempo real

**Demonstra:**
- ‚úÖ Contextual Multi-Armed Bandits
- ‚úÖ Exploration vs Exploitation (Thompson Sampling, UCB)
- ‚úÖ Ensemble de estrat√©gias (Thompson, UCB, Epsilon-Greedy, RL)
- ‚úÖ A/B testing inteligente
- ‚úÖ Cold-start problem

**Executar:**
```bash
python examples/advanced/04_contextual_bandits_personalization.py
```

**O que voc√™ vai aprender:**
- Implementar bandits contextuais
- Combinar m√∫ltiplas estrat√©gias de explora√ß√£o
- Realizar A/B testing adaptativo
- Personalizar conte√∫do por usu√°rio
- Comparar Thompson Sampling vs UCB vs RL

**Casos de uso:**
- Recomenda√ß√£o de produtos
- Personaliza√ß√£o de emails/banners
- Sele√ß√£o de conte√∫do din√¢mico

---

### 5. ü§ù Multi-Agent RL (`05_multi_agent_supply_chain.py`)

**Problema:** Cadeia de suprimentos com m√∫ltiplos agentes cooperando

**Demonstra:**
- ‚úÖ Multi-Agent Reinforcement Learning (MARL)
- ‚úÖ Comunica√ß√£o entre agentes
- ‚úÖ 3 agentes cooperando (Fornecedor, Distribuidor, Varejista)
- ‚úÖ Coordena√ß√£o e troca de mensagens
- ‚úÖ An√°lise de Bullwhip Effect

**Executar:**
```bash
python examples/advanced/05_multi_agent_supply_chain.py
```

**O que voc√™ vai aprender:**
- Treinar m√∫ltiplos agentes simultaneamente
- Implementar comunica√ß√£o entre agentes
- Coordenar decis√µes distribu√≠das
- Minimizar efeito cascata (Bullwhip)
- Credit assignment em MARL

**Casos de uso:**
- Supply chain management
- Sistemas distribu√≠dos
- Leil√µes multi-agente
- Jogos cooperativos

---

### 6. üèóÔ∏è Hierarchical RL (`06_hierarchical_trading_system.py`)

**Problema:** Sistema de trading com decis√µes hier√°rquicas

**Demonstra:**
- ‚úÖ Hierarchical Reinforcement Learning (HRL)
- ‚úÖ Meta-Controller (estrat√©gia de alto n√≠vel)
- ‚úÖ Controllers (execu√ß√£o t√°tica)
- ‚úÖ Temporal abstractions (op√ß√µes/skills)
- ‚úÖ Decomposi√ß√£o hier√°rquica de problemas complexos

**Estrutura:**
```
Meta-Controller ‚Üí Escolhe ESTRAT√âGIA (Agressiva/Moderada/Conservadora)
      ‚Üì
Controllers ‚Üí Executam A√á√ïES espec√≠ficas (compra/venda, stop loss)
```

**Executar:**
```bash
python examples/advanced/06_hierarchical_trading_system.py
```

**O que voc√™ vai aprender:**
- Decompor problemas complexos em hierarquias
- Treinar pol√≠ticas de alto e baixo n√≠vel
- Usar temporal abstractions
- Adaptar estrat√©gias ao contexto

**Casos de uso:**
- Trading automatizado
- Rob√≥tica (planejamento hier√°rquico)
- Jogos complexos
- Navega√ß√£o aut√¥noma

---

### 7. üé≤ Ensemble Learning (`07_ensemble_model_selection.py`)

**Problema:** Combinar m√∫ltiplos agentes RL para robustez

**Demonstra:**
- ‚úÖ Ensemble de agentes com diferentes configura√ß√µes
- ‚úÖ Voting (uniforme e ponderado)
- ‚úÖ Stacking (meta-learning)
- ‚úÖ Mixture of Experts
- ‚úÖ Dynamic model selection
- ‚úÖ An√°lise de diversidade

**T√©cnicas:**
1. **Voting Ensemble:** voto majorit√°rio/ponderado
2. **Stacking:** meta-modelo aprende a combinar
3. **Dynamic Selection:** escolhe melhor modelo por contexto
4. **Mixture of Experts:** combina especializa√ß√µes

**Executar:**
```bash
python examples/advanced/07_ensemble_model_selection.py
```

**O que voc√™ vai aprender:**
- Treinar ensemble de agentes
- Combinar predi√ß√µes de m√∫ltiplos modelos
- Usar meta-learning para sele√ß√£o
- Avaliar diversidade do ensemble
- Aumentar robustez via ensemble

**Casos de uso:**
- Preven√ß√£o de churn
- Detec√ß√£o de fraude
- Sistemas cr√≠ticos (sa√∫de, finan√ßas)
- Decis√µes de alto risco

---

## üöÄ Como Executar os Exemplos

### Pr√©-requisitos

1. **Instalar o Business-RL:**
```bash
pip install git+https://github.com/cbaracho200/Reinforcement_Learning_version_000.git
```

2. **Ou instalar localmente:**
```bash
git clone https://github.com/cbaracho200/Reinforcement_Learning_version_000.git
cd Reinforcement_Learning_version_000
pip install -e .
```

### Executar um exemplo

```bash
# Navegar at√© a pasta do projeto
cd Reinforcement_Learning_version_000

# Exemplos b√°sicos
python examples/advanced/01_portfolio_management.py
python examples/advanced/02_dynamic_pricing.py
python examples/advanced/03_inventory_management.py

# Exemplos avan√ßados (t√©cnicas modernas)
python examples/advanced/04_contextual_bandits_personalization.py
python examples/advanced/05_multi_agent_supply_chain.py
python examples/advanced/06_hierarchical_trading_system.py
python examples/advanced/07_ensemble_model_selection.py
```

### Ajustar tempo de treino

Por padr√£o, os exemplos treinam por 2 horas. Para testes r√°pidos, edite o arquivo e mude:

```python
# De:
modelo = brl.train(problema, hours=2)

# Para (teste r√°pido - 10 minutos):
modelo = brl.train(problema, hours=0.17)
```

---

## üìä Estrutura dos Exemplos

Todos os exemplos seguem a mesma estrutura:

```python
# 1. Defini√ß√£o do problema
@brl.problem(name="NomeProblema")
class MeuProblema:
    obs = brl.Dict(...)      # Observa√ß√µes
    action = brl.Dict(...)   # A√ß√µes
    objectives = brl.Terms(...)  # Objetivos
    constraints = {...}      # Restri√ß√µes
    risk = brl.CVaR(...)    # Gest√£o de risco (opcional)

    # Fun√ß√µes de recompensa
    def reward_objetivo1(self, state, action, next_state):
        ...

    def reward_objetivo2(self, state, action, next_state):
        ...

# 2. Fun√ß√£o de treino
def treinar_modelo():
    problema = MeuProblema()
    modelo = brl.train(problema, algorithm='PPO', hours=2)
    modelo.save('./modelos/meu_modelo.pt')
    return modelo

# 3. Fun√ß√£o de teste
def testar_modelo():
    modelo = brl.load('./modelos/meu_modelo.pt')
    # Testa com v√°rios cen√°rios
    ...

# 4. Execu√ß√£o
if __name__ == "__main__":
    modelo = treinar_modelo()
    testar_modelo()
```

---

## üéØ Conceitos Demonstrados

### 1. Tipos de Observa√ß√£o
- **Cont√≠nuas:** `brl.Box(0, 100)` - valores num√©ricos
- **Discretas:** `brl.Discrete(5)` - categorias
- **Vetoriais:** `brl.Box(0, 1, shape=(10,))` - arrays
- **Dicion√°rios:** `brl.Dict(...)` - m√∫ltiplas observa√ß√µes

### 2. Tipos de A√ß√£o
- **Discretas:** escolher entre op√ß√µes
- **Cont√≠nuas:** valores num√©ricos
- **H√≠bridas:** combina√ß√£o de discretas e cont√≠nuas

### 3. Restri√ß√µes
- **Hard:** nunca pode violar (a√ß√£o inv√°lida)
- **Soft:** pode violar com penalidade
- **Din√¢micas:** dependem do estado

### 4. Algoritmos
- **PPO:** recomendado para iniciantes, est√°vel
- **SAC:** melhor para a√ß√µes cont√≠nuas complexas

### 5. Gest√£o de Risco
- **CVaR:** considera piores cen√°rios
- **Max Drawdown:** limita perdas m√°ximas

### 6. T√©cnicas Avan√ßadas ‚≠ê NOVO

#### Contextual Bandits
- **Exploration vs Exploitation:** balan√ßo entre explorar e exploitar
- **Thompson Sampling:** amostragem bayesiana
- **UCB (Upper Confidence Bound):** intervalos de confian√ßa
- **Epsilon-Greedy:** explora√ß√£o aleat√≥ria

#### Multi-Agent RL
- **Comunica√ß√£o entre agentes:** troca de mensagens
- **Coordena√ß√£o descentralizada:** decis√µes distribu√≠das
- **Credit assignment:** atribuir recompensa a agentes
- **Emerg√™ncia:** comportamento cooperativo emergente

#### Hierarchical RL
- **Meta-Controller:** decis√µes estrat√©gicas de alto n√≠vel
- **Controllers:** execu√ß√£o t√°tica de baixo n√≠vel
- **Temporal abstractions:** skills e op√ß√µes reutiliz√°veis
- **Decomposi√ß√£o hier√°rquica:** dividir problemas complexos

#### Ensemble Learning
- **Voting:** combina√ß√£o por vota√ß√£o
- **Stacking:** meta-modelo aprende a combinar
- **Mixture of Experts:** especialistas para diferentes contextos
- **Dynamic Selection:** escolhe modelo por situa√ß√£o
- **Diversidade:** import√¢ncia de modelos diferentes

---

## üÜö Quando Usar Cada T√©cnica

| T√©cnica | Quando Usar | Exemplo |
|---------|-------------|---------|
| **Contextual Bandits** | Decis√µes independentes, feedback imediato | Recomenda√ß√£o de produtos, A/B testing |
| **Multi-Agent RL** | M√∫ltiplos tomadores de decis√£o cooperando | Supply chain, leil√µes, jogos |
| **Hierarchical RL** | Problemas com m√∫ltiplos n√≠veis de abstra√ß√£o | Trading, rob√≥tica, planejamento |
| **Ensemble** | Aumentar robustez e performance | Sistemas cr√≠ticos, detec√ß√£o de fraude |

---

## üí° Dicas para Usar os Exemplos

### 1. Come√ßar Simples
```bash
# Execute primeiro o exemplo mais simples
python examples/advanced/01_portfolio_management.py
```

### 2. Experimentar com Par√¢metros
```python
# Teste diferentes configura√ß√µes
modelo = brl.train(
    problema,
    algorithm='PPO',
    hours=1,  # Reduza para testes
    config={
        'learning_rate': 3e-4,  # Ajuste conforme necess√°rio
        'batch_size': 128,      # Reduza se tiver pouca mem√≥ria
    }
)
```

### 3. Criar Seus Pr√≥prios Cen√°rios
```python
# Adicione seus pr√≥prios casos de teste
cenarios = [
    {
        'nome': 'Meu Cen√°rio',
        'estado': {
            # Seus dados aqui
        }
    }
]
```

### 4. Comparar com Baseline
```python
# Sempre compare com uma estrat√©gia simples
def estrategia_simples(estado):
    return acao_padrao

# Compare resultados
resultado_rl = testar_modelo_rl()
resultado_baseline = testar_baseline()
print(f"Melhoria: {(resultado_rl/resultado_baseline - 1)*100:.1f}%")
```

---

## üîß Troubleshooting

### Erro: "M√≥dulo business_rl n√£o encontrado"
```bash
# Certifique-se de ter instalado
pip install git+https://github.com/cbaracho200/Reinforcement_Learning_version_000.git
```

### Treino muito lento
```python
# Reduza o tempo de treino
modelo = brl.train(problema, hours=0.5)  # 30 minutos

# Ou reduza batch_size
config={'batch_size': 64}
```

### Modelo n√£o aprende
```python
# 1. Verifique as recompensas
print(problema.reward_objetivo1(estado, acao, proximo_estado))

# 2. Aumente explora√ß√£o
config={'ent_coef': 0.1}

# 3. Reduza learning rate
config={'learning_rate': 1e-4}
```

---

## üìö Pr√≥ximos Passos

1. ‚úÖ Execute todos os exemplos
2. üìù Modifique um exemplo para seu caso de uso
3. üéØ Crie seu pr√≥prio problema do zero
4. üöÄ Compartilhe seus resultados!

---

## ü§ù Contribuindo

Tem um exemplo interessante? Contribua!

1. Fork o reposit√≥rio
2. Crie seu exemplo em `examples/advanced/`
3. Siga a estrutura dos exemplos existentes
4. Envie um Pull Request

---

## üìû Precisa de Ajuda?

- üìö Veja a documenta√ß√£o principal no `README.md`
- üí° Consulte a se√ß√£o "Como Desenvolver Modelos Passo a Passo"
- üêõ Reporte problemas no GitHub Issues

**Boa sorte! üéâ**
