"""
Exemplo Avan√ßado 6: Hierarchical RL - Sistema de Trading

Este exemplo demonstra:
- Hierarchical Reinforcement Learning (HRL)
- Agente de alto n√≠vel (meta-controller): estrat√©gia geral
- Agentes de baixo n√≠vel (controllers): execu√ß√£o t√°tica
- Temporal abstractions (op√ß√µes/skills)
- Decomposi√ß√£o hier√°rquica de decis√µes complexas

Hierarquia:
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   META-CONTROLLER (Alto N√≠vel)      ‚îÇ
‚îÇ   Decide: ESTRAT√âGIA do dia         ‚îÇ
‚îÇ   - Agressiva (compra forte)        ‚îÇ
‚îÇ   - Moderada (balan√ßo)              ‚îÇ
‚îÇ   - Conservadora (prote√ß√£o)         ‚îÇ
‚îÇ   - Liquida√ß√£o (vender tudo)        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
           ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   CONTROLLERS (Baixo N√≠vel)         ‚îÇ
‚îÇ   Executam: A√á√ïES espec√≠ficas       ‚îÇ
‚îÇ   - Quanto comprar/vender           ‚îÇ
‚îÇ   - Que ativos                      ‚îÇ
‚îÇ   - Stop loss / take profit         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
"""

import business_rl as brl
import numpy as np
from typing import Dict, List, Optional
from enum import Enum


class TradingStrategy(Enum):
    """Estrat√©gias de alto n√≠vel."""
    AGRESSIVA = 0      # Alta exposi√ß√£o, busca ganhos
    MODERADA = 1       # Balan√ßo risco/retorno
    CONSERVADORA = 2   # Prote√ß√£o de capital
    LIQUIDACAO = 3     # Reduz posi√ß√µes


# ===== META-CONTROLLER (ALTO N√çVEL) =====
@brl.problem(name="MetaController")
class MetaController:
    """
    Agente de alto n√≠vel: escolhe a ESTRAT√âGIA geral para o per√≠odo.

    Decide a cada N horas qual estrat√©gia seguir,
    baseado em condi√ß√µes macro do mercado.
    """

    obs = brl.Dict(
        # Condi√ß√µes macro do mercado
        volatilidade_mercado=brl.Box(0, 1),  # VIX normalizado
        tendencia_mercado=brl.Box(-1, 1),    # -1=baixa, +1=alta
        volume_mercado=brl.Box(0, 1),        # Volume normalizado

        # Retornos m√©dios dos √≠ndices (agregados)
        sp500_retorno_medio=brl.Box(-0.1, 0.1),
        nasdaq_retorno_medio=brl.Box(-0.1, 0.1),

        # Estado do portf√≥lio
        valor_portfolio=brl.Box(0, 1000000),
        exposicao_atual=brl.Box(0, 1),  # % capital investido
        posicoes_abertas=brl.Box(0, 20),

        # Performance recente
        retorno_7d=brl.Box(-0.5, 0.5),
        retorno_30d=brl.Box(-0.5, 0.5),
        sharpe_ratio=brl.Box(-3, 3),
        max_drawdown=brl.Box(0, 1),

        # Sentimento de mercado
        fear_greed_index=brl.Box(0, 100),
        put_call_ratio=brl.Box(0, 2),

        # Calend√°rio
        hora_dia=brl.Discrete(24),
        dia_semana=brl.Discrete(7),
        fim_trimestre=brl.Discrete(2),  # Pr√≥ximo ao fim?

        # Restri√ß√µes
        margem_disponivel=brl.Box(0, 1),
        limite_risco_usado=brl.Box(0, 1)  # % do limite usado
    )

    # Escolhe uma das 4 estrat√©gias
    action = brl.Discrete(
        4,
        labels=["agressiva", "moderada", "conservadora", "liquidacao"]
    )

    objectives = brl.Terms(
        retorno_ajustado=0.50,    # Retorno ajustado por risco
        consistencia=0.25,         # Volatilidade dos retornos
        protecao_capital=0.15,     # Evitar grandes perdas
        adaptabilidade=0.10        # Resposta a mudan√ßas
    )

    def reward_retorno_ajustado(self, state, action, next_state):
        """Retorno ajustado pelo risco (Sharpe-like)."""
        retorno = next_state['retorno_7d']
        sharpe = next_state['sharpe_ratio']

        # Estrat√©gias diferentes t√™m diferentes perfis
        if action == TradingStrategy.AGRESSIVA.value:
            # Recompensa retornos positivos, tolera volatilidade
            return retorno * 200 + sharpe * 10

        elif action == TradingStrategy.MODERADA.value:
            # Balan√ßo
            return retorno * 150 + sharpe * 20

        elif action == TradingStrategy.CONSERVADORA.value:
            # Prioriza Sharpe sobre retorno absoluto
            return retorno * 100 + sharpe * 30

        else:  # LIQUIDACAO
            # Recompensa redu√ß√£o de exposi√ß√£o
            reducao_exposicao = (state['exposicao_atual'] -
                                next_state['exposicao_atual'])
            return reducao_exposicao * 100

    def reward_consistencia(self, state, action, next_state):
        """Penaliza volatilidade excessiva."""
        volatilidade = state['volatilidade_mercado']

        # Em alta volatilidade, ser conservador √© bom
        if volatilidade > 0.7:
            if action == TradingStrategy.CONSERVADORA.value:
                return 30
            elif action == TradingStrategy.AGRESSIVA.value:
                return -20

        return 0

    def reward_protecao_capital(self, state, action, next_state):
        """Evita grandes perdas."""
        drawdown = next_state['max_drawdown']

        if drawdown > 0.2:  # >20% de perda
            return -100
        elif drawdown > 0.1:  # >10% de perda
            return -30
        else:
            return 10

    def reward_adaptabilidade(self, state, action, next_state):
        """Recompensa adaptar-se √†s condi√ß√µes."""
        tendencia = state['tendencia_mercado']
        volatilidade = state['volatilidade_mercado']

        # Em alta tend√™ncia + baixa vol: ser agressivo
        if tendencia > 0.5 and volatilidade < 0.3:
            return 20 if action == TradingStrategy.AGRESSIVA.value else -10

        # Em baixa tend√™ncia + alta vol: ser conservador
        if tendencia < -0.3 and volatilidade > 0.6:
            return 20 if action == TradingStrategy.CONSERVADORA.value else -10

        return 0


# ===== CONTROLLER: ESTRAT√âGIA AGRESSIVA =====
@brl.problem(name="ControllerAgressivo")
class ControllerAgressivo:
    """
    Executa estrat√©gia agressiva:
    - Alta exposi√ß√£o
    - Busca ativos com momentum
    - Stop loss largo
    """

    obs = brl.Dict(
        # M√©tricas agregadas do portf√≥lio
        preco_medio=brl.Box(0, 500),
        retorno_1h_medio=brl.Box(-0.1, 0.1),
        retorno_24h_medio=brl.Box(-0.3, 0.3),

        # Momentum e volume m√©dio
        rsi_medio=brl.Box(0, 100),  # Relative Strength Index m√©dio
        volume_relativo_medio=brl.Box(0, 5),

        # Posi√ß√µes agregadas
        n_posicoes_long=brl.Box(0, 10),
        n_posicoes_short=brl.Box(0, 10),
        pnl_total=brl.Box(-50000, 50000),

        # Capital dispon√≠vel
        capital_disponivel=brl.Box(0, 1000000),
        margem_disponivel=brl.Box(0, 1)
    )

    action = brl.Dict(
        # A√ß√£o de trading: -1=vender tudo, 0=manter, +1=comprar max
        acao_trading=brl.Box(-1, 1),

        # Stop loss (% de perda aceit√°vel)
        stop_loss=brl.Box(0.05, 0.20),  # 5% a 20%

        # Take profit (% de ganho alvo)
        take_profit=brl.Box(0.10, 0.50),  # 10% a 50%

        # Tamanho da posi√ß√£o (% do capital por trade)
        tamanho_posicao=brl.Box(0.1, 0.5)  # 10% a 50%
    )

    objectives = brl.Terms(
        retorno=0.60,
        momentum=0.25,
        gestao_risco=0.15
    )

    def reward_retorno(self, state, action, next_state):
        """Maximiza retornos."""
        # PnL total
        pnl = next_state['pnl_total']
        return pnl / 100

    def reward_momentum(self, state, action, next_state):
        """Segue momentum."""
        # Alinha a√ß√£o com RSI m√©dio
        acao = action['acao_trading']
        rsi = state['rsi_medio']

        # Recompensa alinhar a√ß√£o com momentum
        # RSI > 50 = momentum positivo, deve comprar
        momentum_signal = (rsi - 50) / 50  # -1 a +1
        alinhamento = acao * momentum_signal
        return alinhamento * 10

    def reward_gestao_risco(self, state, action, next_state):
        """Gest√£o de risco adequada."""
        # Penaliza stop loss muito apertado ou muito largo
        stop = action['stop_loss']
        if 0.08 <= stop <= 0.15:  # Range ideal
            return 10
        else:
            return -5


# ===== CONTROLLER: ESTRAT√âGIA CONSERVADORA =====
@brl.problem(name="ControllerConservador")
class ControllerConservador:
    """
    Executa estrat√©gia conservadora:
    - Baixa exposi√ß√£o
    - Foca em prote√ß√£o de capital
    - Stop loss apertado
    """

    obs = brl.Dict(
        # M√©tricas de risco agregadas
        preco_medio=brl.Box(0, 500),
        volatilidade_media=brl.Box(0, 1),
        beta_portfolio=brl.Box(-2, 2),  # Beta vs mercado

        # Posi√ß√µes agregadas
        n_posicoes_total=brl.Box(0, 10),
        pnl_total=brl.Box(-50000, 50000),

        # Diversifica√ß√£o (simplificado)
        concentracao_portfolio=brl.Box(0, 1),  # 0=diverso, 1=concentrado

        capital_disponivel=brl.Box(0, 1000000)
    )

    action = brl.Dict(
        # A√ß√£o conservadora: menor range
        acao_trading=brl.Box(-0.5, 0.5),  # Menor range

        # Stop loss apertado
        stop_loss=brl.Box(0.03, 0.08),

        # Tamanho menor
        tamanho_posicao=brl.Box(0.05, 0.20)  # 5% a 20%
    )

    objectives = brl.Terms(
        preservacao_capital=0.50,
        diversificacao=0.30,
        retorno=0.20
    )

    def reward_preservacao_capital(self, state, action, next_state):
        """Evita perdas."""
        pnl = next_state['pnl_total']

        if pnl < 0:
            return pnl / 10  # Penaliza perdas fortemente
        else:
            return pnl / 50  # Retorno modesto ok

    def reward_diversificacao(self, state, action, next_state):
        """Incentiva diversifica√ß√£o."""
        # Usa concentra√ß√£o do portf√≥lio (0=diverso, 1=concentrado)
        concentracao = state['concentracao_portfolio']

        # Menor concentra√ß√£o = melhor
        return (1 - concentracao) * 50

    def reward_retorno(self, state, action, next_state):
        """Retorno modesto."""
        pnl = next_state['pnl_total']
        return max(0, pnl / 100)  # S√≥ recompensa ganhos


class HierarchicalTradingSystem:
    """Sistema de trading hier√°rquico."""

    def __init__(self):
        self.meta_controller = None
        self.controller_agressivo = None
        self.controller_conservador = None

        # Estado atual
        self.estrategia_atual = None
        self.tempo_na_estrategia = 0

        # Hist√≥rico
        self.historico = []

    def treinar_hierarquia(self):
        """Treina todos os n√≠veis da hierarquia."""
        print("=" * 70)
        print("TREINAMENTO: HIERARCHICAL RL - TRADING SYSTEM")
        print("=" * 70)

        # 1. Treina Meta-Controller
        print("\nüéØ Treinando Meta-Controller (alto n√≠vel)...")
        problema_meta = MetaController()
        self.meta_controller = brl.train(
            problema_meta,
            algorithm='PPO',
            hours=0.5,
            config={'learning_rate': 3e-4, 'gamma': 0.99}
        )
        self.meta_controller.save('./modelos/meta_controller.pt')

        # 2. Treina Controller Agressivo
        print("\n‚ö° Treinando Controller Agressivo...")
        problema_agressivo = ControllerAgressivo()
        self.controller_agressivo = brl.train(
            problema_agressivo,
            algorithm='PPO',
            hours=0.5,
            config={'learning_rate': 3e-4, 'gamma': 0.95}
        )
        self.controller_agressivo.save('./modelos/controller_agressivo.pt')

        # 3. Treina Controller Conservador
        print("\nüõ°Ô∏è  Treinando Controller Conservador...")
        problema_conservador = ControllerConservador()
        self.controller_conservador = brl.train(
            problema_conservador,
            algorithm='PPO',
            hours=0.5,
            config={'learning_rate': 3e-4, 'gamma': 0.95}
        )
        self.controller_conservador.save('./modelos/controller_conservador.pt')

        print("\n‚úÖ Hierarquia completa treinada!")

    def carregar_hierarquia(self):
        """Carrega modelos treinados."""
        self.meta_controller = brl.load('./modelos/meta_controller.pt')
        self.controller_agressivo = brl.load('./modelos/controller_agressivo.pt')
        self.controller_conservador = brl.load('./modelos/controller_conservador.pt')

    def decidir_estrategia(self, estado_macro):
        """Meta-controller decide a estrat√©gia."""
        decisao = self.meta_controller.decide(estado_macro, deterministic=True)
        self.estrategia_atual = TradingStrategy(decisao.action)
        self.tempo_na_estrategia = 0

        return self.estrategia_atual

    def executar_estrategia(self, estado_mercado):
        """Controller apropriado executa a estrat√©gia."""
        if self.estrategia_atual == TradingStrategy.AGRESSIVA:
            decisao = self.controller_agressivo.decide(
                estado_mercado, deterministic=True
            )

        elif self.estrategia_atual == TradingStrategy.CONSERVADORA:
            decisao = self.controller_conservador.decide(
                estado_mercado, deterministic=True
            )

        elif self.estrategia_atual == TradingStrategy.MODERADA:
            # Mix de agressivo e conservador
            decisao_agr = self.controller_agressivo.decide(estado_mercado)
            decisao_cons = self.controller_conservador.decide(estado_mercado)

            # M√©dia ponderada
            decisao = type('obj', (object,), {
                'action': {
                    'acao_trading': (decisao_agr.action['acao_trading'] * 0.5 +
                                    decisao_cons.action['acao_trading'] * 0.5),
                    'stop_loss': (decisao_agr.action['stop_loss'] +
                                 decisao_cons.action['stop_loss']) / 2,
                    'tamanho_posicao': (decisao_agr.action['tamanho_posicao'] +
                                       decisao_cons.action['tamanho_posicao']) / 2
                }
            })()

        else:  # LIQUIDACAO
            # Fecha todas as posi√ß√µes
            decisao = type('obj', (object,), {
                'action': {
                    'acao_trading': 0.0,
                    'stop_loss': 0.05,
                    'tamanho_posicao': 0.0
                }
            })()

        self.tempo_na_estrategia += 1
        return decisao


def demo_hierarchical_trading():
    """Demonstra√ß√£o do sistema hier√°rquico."""
    print("\n" + "=" * 70)
    print("DEMONSTRA√á√ÉO: TRADING HIER√ÅRQUICO")
    print("=" * 70)

    # Cria sistema
    sistema = HierarchicalTradingSystem()
    sistema.carregar_hierarquia()

    # Simula 10 ciclos de decis√£o
    print("\nüìä Simulando 10 ciclos de decis√£o...\n")

    for ciclo in range(10):
        print(f"\n{'='*70}")
        print(f"CICLO {ciclo + 1}")
        print(f"{'='*70}")

        # Estado macro (aleat√≥rio para demo)
        estado_macro = {
            'volatilidade_mercado': np.random.rand(),
            'tendencia_mercado': np.random.randn() * 0.5,
            'volume_mercado': np.random.rand(),
            'sp500_retorno_medio': np.random.randn() * 0.02,
            'nasdaq_retorno_medio': np.random.randn() * 0.025,
            'valor_portfolio': 100000,
            'exposicao_atual': np.random.rand() * 0.8,
            'posicoes_abertas': np.random.randint(5, 15),
            'retorno_7d': np.random.randn() * 0.1,
            'retorno_30d': np.random.randn() * 0.2,
            'sharpe_ratio': np.random.randn(),
            'max_drawdown': np.random.rand() * 0.15,
            'fear_greed_index': np.random.randint(20, 80),
            'put_call_ratio': 0.8 + np.random.rand() * 0.4,
            'hora_dia': np.random.randint(24),
            'dia_semana': np.random.randint(7),
            'fim_trimestre': 0,
            'margem_disponivel': np.random.rand(),
            'limite_risco_usado': np.random.rand() * 0.7
        }

        # Meta-controller decide estrat√©gia
        estrategia = sistema.decidir_estrategia(estado_macro)
        print(f"\nüéØ ESTRAT√âGIA ESCOLHIDA: {estrategia.name}")
        print(f"   Volatilidade: {estado_macro['volatilidade_mercado']:.2%}")
        print(f"   Tend√™ncia: {estado_macro['tendencia_mercado']:+.2%}")
        print(f"   Exposi√ß√£o atual: {estado_macro['exposicao_atual']:.2%}")

        # Estado do mercado (agregado)
        estado_mercado = {
            'preco_medio': np.random.rand() * 100 + 50,
            'retorno_1h_medio': np.random.randn() * 0.02,
            'retorno_24h_medio': np.random.randn() * 0.05,
            'rsi_medio': np.random.rand() * 100,
            'volume_relativo_medio': 0.5 + np.random.rand(),
            'n_posicoes_long': np.random.randint(0, 5),
            'n_posicoes_short': np.random.randint(0, 3),
            'pnl_total': np.random.randn() * 2500,
            'capital_disponivel': 100000,
            'margem_disponivel': 0.8,
            'volatilidade_media': np.random.rand() * 0.5,
            'beta_portfolio': np.random.randn() * 0.5 + 1,
            'concentracao_portfolio': np.random.rand() * 0.5,
            'n_posicoes_total': np.random.randint(1, 8)
        }

        # Executa estrat√©gia
        decisao = sistema.executar_estrategia(estado_mercado)

        print(f"\nüìà A√á√ÉO EXECUTADA:")
        if 'acao_trading' in decisao.action:
            acao = decisao.action['acao_trading']
            if abs(acao) > 0.1:
                operacao = "COMPRA" if acao > 0 else "VENDA"
                intensidade = abs(acao)
                print(f"   {operacao} (intensidade: {intensidade:.2f})")
            else:
                print(f"   MANTER (neutro)")
        else:
            print(f"   Estrat√©gia de LIQUIDA√á√ÉO")

        if hasattr(decisao.action, 'stop_loss'):
            print(f"\n   Stop Loss: {decisao.action['stop_loss']:.1%}")
            print(f"   Tamanho Posi√ß√£o: {decisao.action['tamanho_posicao']:.1%}")


if __name__ == "__main__":
    import os
    os.makedirs('./modelos', exist_ok=True)

    print("üöÄ Iniciando exemplo Hierarchical RL\n")

    # Cria e treina sistema
    sistema = HierarchicalTradingSystem()
    sistema.treinar_hierarquia()

    # Demonstra√ß√£o
    demo_hierarchical_trading()

    print("\n" + "=" * 70)
    print("‚úÖ Exemplo conclu√≠do!")
    print("=" * 70)
